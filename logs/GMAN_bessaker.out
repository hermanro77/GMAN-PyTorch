compute-4-0-19.foreman.hpc.ntnu.no
K=8, L=1, SE_file='./data/SE_Bessaker.txt', batch_size=32, d=8, decay_epoch=10, learning_rate=0.001, log_file='./data/log', max_epoch=1, model_file='./GMAN_Bessaker.pkl', num_his=12, num_pred=12, patience=10, test_ratio=0.2, time_slot=5, traffic_file='./data/Bessaker.h5', train_ratio=0.7, val_ratio=0.1
loading data...
TIME:  tensor([[3, 0],
        [3, 0],
        [3, 0],
        ...,
        [3, 0],
        [3, 0],
        [3, 0]])
trainX: torch.Size([40709, 12, 25])		 trainY: torch.Size([40709, 12, 25])
valX:   torch.Size([5796, 12, 25])		valY:   torch.Size([5796, 12, 25])
testX:   torch.Size([11615, 12, 25])		testY:   torch.Size([11615, 12, 25])
mean:   0.7267		std:   0.7864
data loaded!
compiling model...
trainable parameters: 209,923
TIME:  tensor([[3, 0],
        [3, 0],
        [3, 0],
        ...,
        [3, 0],
        [3, 0],
        [3, 0]])
**** training model ****
Starting batches..  1273
Training batch: 5 in epoch:0, training batch loss:0.5567
Training batch: 10 in epoch:0, training batch loss:0.4160
Training batch: 15 in epoch:0, training batch loss:0.5598
Training batch: 20 in epoch:0, training batch loss:0.4921
Training batch: 25 in epoch:0, training batch loss:0.4156
Training batch: 30 in epoch:0, training batch loss:0.4346
Training batch: 35 in epoch:0, training batch loss:0.4360
Training batch: 40 in epoch:0, training batch loss:0.2826
Training batch: 45 in epoch:0, training batch loss:0.4068
Training batch: 50 in epoch:0, training batch loss:0.5114
Training batch: 55 in epoch:0, training batch loss:0.5911
Training batch: 60 in epoch:0, training batch loss:0.4253
Training batch: 65 in epoch:0, training batch loss:0.2917
Training batch: 70 in epoch:0, training batch loss:0.4609
Training batch: 75 in epoch:0, training batch loss:0.5308
Training batch: 80 in epoch:0, training batch loss:0.4494
Training batch: 85 in epoch:0, training batch loss:0.4757
Training batch: 90 in epoch:0, training batch loss:0.5521
Training batch: 95 in epoch:0, training batch loss:0.4768
Training batch: 100 in epoch:0, training batch loss:0.5322
Training batch: 105 in epoch:0, training batch loss:0.2192
Training batch: 110 in epoch:0, training batch loss:0.6082
Training batch: 115 in epoch:0, training batch loss:0.5572
Training batch: 120 in epoch:0, training batch loss:0.4033
Training batch: 125 in epoch:0, training batch loss:0.5515
Training batch: 130 in epoch:0, training batch loss:0.5025
Training batch: 135 in epoch:0, training batch loss:0.4266
Training batch: 140 in epoch:0, training batch loss:0.2816
Training batch: 145 in epoch:0, training batch loss:0.5077
Training batch: 150 in epoch:0, training batch loss:0.3811
Training batch: 155 in epoch:0, training batch loss:0.3093
Training batch: 160 in epoch:0, training batch loss:0.3957
Training batch: 165 in epoch:0, training batch loss:0.3058
Training batch: 170 in epoch:0, training batch loss:0.3257
Training batch: 175 in epoch:0, training batch loss:0.3694
Training batch: 180 in epoch:0, training batch loss:0.3054
Training batch: 185 in epoch:0, training batch loss:0.3815
Training batch: 190 in epoch:0, training batch loss:0.3288
Training batch: 195 in epoch:0, training batch loss:0.3644
Training batch: 200 in epoch:0, training batch loss:0.3316
Training batch: 205 in epoch:0, training batch loss:0.2971
Training batch: 210 in epoch:0, training batch loss:0.3196
Training batch: 215 in epoch:0, training batch loss:0.3247
Training batch: 220 in epoch:0, training batch loss:0.3300
Training batch: 225 in epoch:0, training batch loss:0.4738
Training batch: 230 in epoch:0, training batch loss:0.4806
Training batch: 235 in epoch:0, training batch loss:0.3855
Training batch: 240 in epoch:0, training batch loss:0.3134
Training batch: 245 in epoch:0, training batch loss:0.4368
Training batch: 250 in epoch:0, training batch loss:0.3301
Training batch: 255 in epoch:0, training batch loss:0.3296
Training batch: 260 in epoch:0, training batch loss:0.3539
Training batch: 265 in epoch:0, training batch loss:0.4652
Training batch: 270 in epoch:0, training batch loss:0.4182
Training batch: 275 in epoch:0, training batch loss:0.3586
Training batch: 280 in epoch:0, training batch loss:0.3540
Training batch: 285 in epoch:0, training batch loss:0.4753
Training batch: 290 in epoch:0, training batch loss:0.4351
Training batch: 295 in epoch:0, training batch loss:0.4142
Training batch: 300 in epoch:0, training batch loss:0.3400
Training batch: 305 in epoch:0, training batch loss:0.3834
Training batch: 310 in epoch:0, training batch loss:0.5663
Training batch: 315 in epoch:0, training batch loss:0.4067
Training batch: 320 in epoch:0, training batch loss:0.2533
Training batch: 325 in epoch:0, training batch loss:0.3229
Training batch: 330 in epoch:0, training batch loss:0.3977
Training batch: 335 in epoch:0, training batch loss:0.2294
Training batch: 340 in epoch:0, training batch loss:0.3599
Training batch: 345 in epoch:0, training batch loss:0.2609
Training batch: 350 in epoch:0, training batch loss:0.2304
Training batch: 355 in epoch:0, training batch loss:0.2636
Training batch: 360 in epoch:0, training batch loss:0.3274
Training batch: 365 in epoch:0, training batch loss:0.4598
Training batch: 370 in epoch:0, training batch loss:0.3136
Training batch: 375 in epoch:0, training batch loss:0.4448
Training batch: 380 in epoch:0, training batch loss:0.3893
Training batch: 385 in epoch:0, training batch loss:0.4834
Training batch: 390 in epoch:0, training batch loss:0.3078
Training batch: 395 in epoch:0, training batch loss:0.2440
Training batch: 400 in epoch:0, training batch loss:0.2922
Training batch: 405 in epoch:0, training batch loss:0.4284
Training batch: 410 in epoch:0, training batch loss:0.4459
Training batch: 415 in epoch:0, training batch loss:0.4814
Training batch: 420 in epoch:0, training batch loss:0.2540
Training batch: 425 in epoch:0, training batch loss:0.3110
Training batch: 430 in epoch:0, training batch loss:0.2819
Training batch: 435 in epoch:0, training batch loss:0.3488
Training batch: 440 in epoch:0, training batch loss:0.4199
Training batch: 445 in epoch:0, training batch loss:0.3479
Training batch: 450 in epoch:0, training batch loss:0.4336
Training batch: 455 in epoch:0, training batch loss:0.3607
Training batch: 460 in epoch:0, training batch loss:0.3909
Training batch: 465 in epoch:0, training batch loss:0.2975
Training batch: 470 in epoch:0, training batch loss:0.3342
Training batch: 475 in epoch:0, training batch loss:0.4232
Training batch: 480 in epoch:0, training batch loss:0.3778
Training batch: 485 in epoch:0, training batch loss:0.2929
Training batch: 490 in epoch:0, training batch loss:0.4769
Training batch: 495 in epoch:0, training batch loss:0.3109
Training batch: 500 in epoch:0, training batch loss:0.3560
Training batch: 505 in epoch:0, training batch loss:0.3694
Training batch: 510 in epoch:0, training batch loss:0.3351
Training batch: 515 in epoch:0, training batch loss:0.4916
Training batch: 520 in epoch:0, training batch loss:0.4462
Training batch: 525 in epoch:0, training batch loss:0.3161
Training batch: 530 in epoch:0, training batch loss:0.2997
Training batch: 535 in epoch:0, training batch loss:0.3086
Training batch: 540 in epoch:0, training batch loss:0.3737
Training batch: 545 in epoch:0, training batch loss:0.3262
Training batch: 550 in epoch:0, training batch loss:0.2401
Training batch: 555 in epoch:0, training batch loss:0.3200
Training batch: 560 in epoch:0, training batch loss:0.3451
Training batch: 565 in epoch:0, training batch loss:0.3116
Training batch: 570 in epoch:0, training batch loss:0.3581
Training batch: 575 in epoch:0, training batch loss:0.2745
Training batch: 580 in epoch:0, training batch loss:0.2655
Training batch: 585 in epoch:0, training batch loss:0.2963
Training batch: 590 in epoch:0, training batch loss:0.4040
Training batch: 595 in epoch:0, training batch loss:0.3064
Training batch: 600 in epoch:0, training batch loss:0.3704
Training batch: 605 in epoch:0, training batch loss:0.3519
Training batch: 610 in epoch:0, training batch loss:0.4146
Training batch: 615 in epoch:0, training batch loss:0.3384
Training batch: 620 in epoch:0, training batch loss:0.4468
Training batch: 625 in epoch:0, training batch loss:0.4924
Training batch: 630 in epoch:0, training batch loss:0.3501
Training batch: 635 in epoch:0, training batch loss:0.2678
Training batch: 640 in epoch:0, training batch loss:0.3565
Training batch: 645 in epoch:0, training batch loss:0.4532
Training batch: 650 in epoch:0, training batch loss:0.3380
Training batch: 655 in epoch:0, training batch loss:0.3785
Training batch: 660 in epoch:0, training batch loss:0.3494
Training batch: 665 in epoch:0, training batch loss:0.3629
Training batch: 670 in epoch:0, training batch loss:0.5031
Training batch: 675 in epoch:0, training batch loss:0.3394
Training batch: 680 in epoch:0, training batch loss:0.2533
Training batch: 685 in epoch:0, training batch loss:0.4799
Training batch: 690 in epoch:0, training batch loss:0.6534
Training batch: 695 in epoch:0, training batch loss:0.3393
Training batch: 700 in epoch:0, training batch loss:0.3631
Training batch: 705 in epoch:0, training batch loss:0.3107
Training batch: 710 in epoch:0, training batch loss:0.4759
Training batch: 715 in epoch:0, training batch loss:0.3339
Training batch: 720 in epoch:0, training batch loss:0.3392
Training batch: 725 in epoch:0, training batch loss:0.3368
Training batch: 730 in epoch:0, training batch loss:0.3095
Training batch: 735 in epoch:0, training batch loss:0.2543
Training batch: 740 in epoch:0, training batch loss:0.3537
Training batch: 745 in epoch:0, training batch loss:0.3598
Training batch: 750 in epoch:0, training batch loss:0.4703
Training batch: 755 in epoch:0, training batch loss:0.2846
Training batch: 760 in epoch:0, training batch loss:0.3329
Training batch: 765 in epoch:0, training batch loss:0.2432
Training batch: 770 in epoch:0, training batch loss:0.4226
Training batch: 775 in epoch:0, training batch loss:0.3893
Training batch: 780 in epoch:0, training batch loss:0.4158
Training batch: 785 in epoch:0, training batch loss:0.2429
Training batch: 790 in epoch:0, training batch loss:0.2840
Training batch: 795 in epoch:0, training batch loss:0.3342
Training batch: 800 in epoch:0, training batch loss:0.3760
Training batch: 805 in epoch:0, training batch loss:0.3501
Training batch: 810 in epoch:0, training batch loss:0.3181
Training batch: 815 in epoch:0, training batch loss:0.3852
Training batch: 820 in epoch:0, training batch loss:0.2964
Training batch: 825 in epoch:0, training batch loss:0.2670
Training batch: 830 in epoch:0, training batch loss:0.3286
Training batch: 835 in epoch:0, training batch loss:0.3031
Training batch: 840 in epoch:0, training batch loss:0.3273
Training batch: 845 in epoch:0, training batch loss:0.4017
Training batch: 850 in epoch:0, training batch loss:0.3617
Training batch: 855 in epoch:0, training batch loss:0.2839
Training batch: 860 in epoch:0, training batch loss:0.3374
Training batch: 865 in epoch:0, training batch loss:0.3547
Training batch: 870 in epoch:0, training batch loss:0.4042
Training batch: 875 in epoch:0, training batch loss:0.3702
Training batch: 880 in epoch:0, training batch loss:0.3986
Training batch: 885 in epoch:0, training batch loss:0.5882
Training batch: 890 in epoch:0, training batch loss:0.2125
Training batch: 895 in epoch:0, training batch loss:0.3266
Training batch: 900 in epoch:0, training batch loss:0.4035
Training batch: 905 in epoch:0, training batch loss:0.5144
Training batch: 910 in epoch:0, training batch loss:0.2759
Training batch: 915 in epoch:0, training batch loss:0.4435
Training batch: 920 in epoch:0, training batch loss:0.3489
Training batch: 925 in epoch:0, training batch loss:0.3216
Training batch: 930 in epoch:0, training batch loss:0.3956
Training batch: 935 in epoch:0, training batch loss:0.4218
Training batch: 940 in epoch:0, training batch loss:0.3808
Training batch: 945 in epoch:0, training batch loss:0.4905
Training batch: 950 in epoch:0, training batch loss:0.3709
Training batch: 955 in epoch:0, training batch loss:0.3268
Training batch: 960 in epoch:0, training batch loss:0.3120
Training batch: 965 in epoch:0, training batch loss:0.3019
Training batch: 970 in epoch:0, training batch loss:0.3601
Training batch: 975 in epoch:0, training batch loss:0.2509
Training batch: 980 in epoch:0, training batch loss:0.3642
Training batch: 985 in epoch:0, training batch loss:0.4213
Training batch: 990 in epoch:0, training batch loss:0.3812
Training batch: 995 in epoch:0, training batch loss:0.3372
Training batch: 1000 in epoch:0, training batch loss:0.2965
Training batch: 1005 in epoch:0, training batch loss:0.3454
Training batch: 1010 in epoch:0, training batch loss:0.2851
Training batch: 1015 in epoch:0, training batch loss:0.4019
Training batch: 1020 in epoch:0, training batch loss:0.3834
Training batch: 1025 in epoch:0, training batch loss:0.3587
Training batch: 1030 in epoch:0, training batch loss:0.5162
Training batch: 1035 in epoch:0, training batch loss:0.2577
Training batch: 1040 in epoch:0, training batch loss:0.2975
Training batch: 1045 in epoch:0, training batch loss:0.4805
Training batch: 1050 in epoch:0, training batch loss:0.3501
Training batch: 1055 in epoch:0, training batch loss:0.3413
Training batch: 1060 in epoch:0, training batch loss:0.3155
Training batch: 1065 in epoch:0, training batch loss:0.4310
Training batch: 1070 in epoch:0, training batch loss:0.2669
Training batch: 1075 in epoch:0, training batch loss:0.5127
Training batch: 1080 in epoch:0, training batch loss:0.3179
Training batch: 1085 in epoch:0, training batch loss:0.3405
Training batch: 1090 in epoch:0, training batch loss:0.3933
Training batch: 1095 in epoch:0, training batch loss:0.3144
Training batch: 1100 in epoch:0, training batch loss:0.3622
Training batch: 1105 in epoch:0, training batch loss:0.3050
Training batch: 1110 in epoch:0, training batch loss:0.2321
Training batch: 1115 in epoch:0, training batch loss:0.3503
Training batch: 1120 in epoch:0, training batch loss:0.2942
Training batch: 1125 in epoch:0, training batch loss:0.4131
Training batch: 1130 in epoch:0, training batch loss:0.3371
Training batch: 1135 in epoch:0, training batch loss:0.3368
Training batch: 1140 in epoch:0, training batch loss:0.5303
Training batch: 1145 in epoch:0, training batch loss:0.3176
Training batch: 1150 in epoch:0, training batch loss:0.3101
Training batch: 1155 in epoch:0, training batch loss:0.3450
Training batch: 1160 in epoch:0, training batch loss:0.4006
Training batch: 1165 in epoch:0, training batch loss:0.2856
Training batch: 1170 in epoch:0, training batch loss:0.3078
Training batch: 1175 in epoch:0, training batch loss:0.3840
Training batch: 1180 in epoch:0, training batch loss:0.3351
Training batch: 1185 in epoch:0, training batch loss:0.3645
Training batch: 1190 in epoch:0, training batch loss:0.4830
Training batch: 1195 in epoch:0, training batch loss:0.3293
Training batch: 1200 in epoch:0, training batch loss:0.2993
Training batch: 1205 in epoch:0, training batch loss:0.3863
Training batch: 1210 in epoch:0, training batch loss:0.3514
Training batch: 1215 in epoch:0, training batch loss:0.3638
Training batch: 1220 in epoch:0, training batch loss:0.3145
Training batch: 1225 in epoch:0, training batch loss:0.3263
Training batch: 1230 in epoch:0, training batch loss:0.2922
Training batch: 1235 in epoch:0, training batch loss:0.3194
Training batch: 1240 in epoch:0, training batch loss:0.3808
Training batch: 1245 in epoch:0, training batch loss:0.2988
Training batch: 1250 in epoch:0, training batch loss:0.3501
Training batch: 1255 in epoch:0, training batch loss:0.4057
Training batch: 1260 in epoch:0, training batch loss:0.3746
Training batch: 1265 in epoch:0, training batch loss:0.2810
Training batch: 1270 in epoch:0, training batch loss:0.2531
2021-02-12 23:51:05 | epoch: 0001/1, training time: 1047.8s, inference time: 51.5s
train loss: 0.3734, val_loss: 0.3994
val loss decrease from inf to 0.3994, saving model to ./GMAN_Bessaker.pkl
Training and validation are completed, and model has been stored as ./GMAN_Bessaker.pkl
TIME:  tensor([[3, 0],
        [3, 0],
        [3, 0],
        ...,
        [3, 0],
        [3, 0],
        [3, 0]])
**** testing model ****
loading model from ./GMAN_Bessaker.pkl
model restored!
evaluating...
testing time: 103.4s
                MAE		RMSE		MAPE
train            0.45		0.60		nan%
val              0.50		0.64		nan%
test             0.47		0.61		nan%
performance in each prediction step
step: 01         0.38		0.49		nan%
step: 02         0.40		0.52		nan%
step: 03         0.42		0.55		nan%
step: 04         0.44		0.57		nan%
step: 05         0.46		0.59		nan%
step: 06         0.47		0.61		nan%
step: 07         0.48		0.63		nan%
step: 08         0.50		0.65		nan%
step: 09         0.51		0.66		nan%
step: 10         0.52		0.67		nan%
step: 11         0.52		0.68		nan%
step: 12         0.53		0.69		nan%
average:         0.47		0.61		nan%
total time: 27.1min
Traceback (most recent call last):
  File "main.py", line 108, in <module>
    c.append(testPred[j, i, k])
IndexError: index 25 is out of bounds for dimension 2 with size 25
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[22661,1],0]
  Exit code:    1
--------------------------------------------------------------------------
